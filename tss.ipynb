{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-12T10:37:58.141265600Z",
     "start_time": "2024-05-12T10:36:43.792740200Z"
    }
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "def unzip_folder(zip_file, extract_to):\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "zip_file = 'val.zip'  \n",
    "extract_to = 'val' \n",
    "\n",
    "unzip_folder(zip_file, extract_to)\n",
    "\n",
    "zip_file = 'test.zip'  \n",
    "extract_to = 'test'  \n",
    "\n",
    "unzip_folder(zip_file, extract_to)\n",
    "\n",
    "zip_file = 'train.zip'  \n",
    "extract_to = 'train_unzipped' \n",
    "\n",
    "#unzip_folder(zip_file, extract_to) \n",
    "#al 3lea deja facut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val done\n",
      "test done\n",
      "train done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ujson\n",
    "\n",
    "def read_json_file(filepath):\n",
    "    try:\n",
    "        with open(filepath, 'rb') as f: \n",
    "            file_content = f.read()\n",
    "            json_data = ujson.loads(file_content)  \n",
    "            filtered_data = {'target': json_data.get('target'), 'src_fm': json_data.get('src_fm')}\n",
    "            return filtered_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {filepath}: {e}\")\n",
    "        return None\n",
    "\n",
    "def read_json_files(directory):\n",
    "    file_paths = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.json'):\n",
    "                file_paths.append(os.path.join(root, file))\n",
    "\n",
    "    results = []\n",
    "    for path in file_paths:\n",
    "        results.append(read_json_file(path))\n",
    "\n",
    "    results = [data for data in results if data is not None]\n",
    "\n",
    "    return results\n",
    "\n",
    "val = read_json_files('val')\n",
    "print(\"val done\")\n",
    "test = read_json_files('test')\n",
    "print(\"test done\")\n",
    "train = read_json_files('train_unzipped')\n",
    "print(\"train done\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-12T15:02:26.268877200Z",
     "start_time": "2024-05-12T13:55:07.401505200Z"
    }
   },
   "id": "ab6511d6d601a08a"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "train_src, train_tests = list(zip(*[(d['src_fm'], d['target']) for d in train]))\n",
    "val_src, val_tests = list(zip(*[(d['src_fm'], d['target']) for d in val]))\n",
    "test_src, test_tests = list(zip(*[(d['src_fm'], d['target']) for d in test]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-12T15:02:26.922985900Z",
     "start_time": "2024-05-12T15:02:26.253917200Z"
    }
   },
   "id": "24a7f1119e8b77da"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'src': train_src, 'tests': train_tests})\n",
    "df.to_csv(\"train.csv\", index=False)\n",
    "df = pd.DataFrame({'src': val_src, 'tests': val_tests})\n",
    "df.to_csv(\"val.csv\", index=False)\n",
    "df = pd.DataFrame({'src': test_src, 'tests': test_tests})\n",
    "df.to_csv(\"test.csv\", index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-12T16:49:40.162706900Z",
     "start_time": "2024-05-12T16:49:22.995962400Z"
    }
   },
   "id": "d2a93691f95533d2"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "val = pd.read_csv('val.csv')\n",
    "train_src, train_tests = list(train['src']), list(train['tests'])\n",
    "val_src, val_tests = list(val['src']), list(val['tests'])\n",
    "test_src, test_tests = list(test['src']), list(test['tests'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-12T13:44:58.854321300Z",
     "start_time": "2024-05-12T13:44:58.792929700Z"
    }
   },
   "id": "e3157a456584d02e"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipykernel in c:\\users\\steve\\anaconda3\\lib\\site-packages (6.28.0)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from ipykernel) (0.2.1)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from ipykernel) (1.6.7)\n",
      "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from ipykernel) (8.20.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from ipykernel) (7.4.9)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from ipykernel) (5.5.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from ipykernel) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\steve\\anaconda3\\lib\\site-packages (from ipykernel) (1.6.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\steve\\anaconda3\\lib\\site-packages (from ipykernel) (23.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\steve\\anaconda3\\lib\\site-packages (from ipykernel) (5.9.0)\n",
      "Requirement already satisfied: pyzmq>=24 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from ipykernel) (24.0.1)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from ipykernel) (6.3.3)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from ipykernel) (5.7.1)\n",
      "Requirement already satisfied: decorator in c:\\users\\steve\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.18.1)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (2.15.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\steve\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\steve\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.4.6)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\steve\\anaconda3\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel) (0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel) (2.8.2)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (3.10.0)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (305.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\steve\\anaconda3\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\steve\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel) (1.16.0)\n",
      "Requirement already satisfied: executing in c:\\users\\steve\\anaconda3\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel) (0.8.3)\n",
      "Requirement already satisfied: asttokens in c:\\users\\steve\\anaconda3\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\steve\\anaconda3\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel) (0.2.2)\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-12T13:54:49.009918500Z",
     "start_time": "2024-05-12T13:54:43.859047300Z"
    }
   },
   "id": "f3246bc473c3eb90"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "\n",
    "max_seq_length = 128\n",
    "train_inputs = [tokenizer.encode(code, max_length=max_seq_length, truncation=True, padding='max_length') for code in train_src]\n",
    "val_inputs = [tokenizer.encode(code, max_length=max_seq_length, truncation=True, padding='max_length') for code in val_src]\n",
    "print(\"truncated\")\n",
    "input_ids_train = torch.tensor(train_inputs)\n",
    "input_ids_val = torch.tensor(val_inputs)\n",
    "\n",
    "train_data = TensorDataset(input_ids_train)\n",
    "train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "epochs = 40\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * epochs)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"i am now reaching the training loop\")\n",
    "for epoch in range(epochs):\n",
    "    print(\"before train\")\n",
    "    model.train()\n",
    "    print(\"in train mode\")\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        inputs = batch[0].to(device)\n",
    "        print(\"inside loop\")\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=inputs, labels=inputs)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        print(\"calculated loss\")\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        print(\"batch done\")\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f'Epoch {epoch + 1}/{epochs}')\n",
    "    print(f'Average Training Loss: {avg_train_loss:.4f}')\n",
    "    \n",
    "#am un gpu foarte prost si nu prea vrea sa se antreneze cat de cat rapid(o epoca ia aprox. 30 de min)\n",
    "#probabil ca se putea face mai bine, dar nu stiu eu.\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-12T16:47:33.321929100Z",
     "start_time": "2024-05-12T16:41:10.794273200Z"
    }
   },
   "id": "544daf74a5ed240c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc3697c88c9743ce"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_inputs = [tokenizer.encode(code, max_length=max_seq_length, truncation=True, padding='max_length') for code in test_src]\n",
    "\n",
    "input_ids_test = torch.tensor(test_inputs).to(device)\n",
    "\n",
    "generated_test_cases = []\n",
    "for input_ids in input_ids_test:\n",
    "    input_ids = input_ids.unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(input_ids=input_ids, max_length=50, num_beams=5)\n",
    "    generated_test_case = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    generated_test_cases.append(generated_test_case)\n",
    "\n",
    "for java_code, generated_test_case in zip(test_java_code, generated_test_cases):\n",
    "    print(f\"Java Code: {java_code}\")\n",
    "    print(f\"Generated Test Case: {generated_test_case}\\n\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3658fcafb03c33b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'tss.pth')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d8042de06f32acc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
